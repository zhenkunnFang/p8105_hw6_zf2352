---
title: "p8105_hw6_zf2352"
author: "Zhenkun Fang"
date: "2024-12-02"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(dplyr)
library(broom)
library(purrr)
library(ggplot2)
library(modelr)
library(p8105.datasets)
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

```{r message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

```

```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}


boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

boot_straps
```

```{r}
bootstrap_results1 = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin, data = df) ),
    results = map(models, broom::glance)) |> 
  select(-strap_sample, -models) |> 
  unnest(results) 


bootstrap_results1 |> 
  summarize(
    rsquare_lower = quantile(r.squared, 0.025), 
    rsquare_upper = quantile(r.squared, 0.975)) %>% 
    knitr::kable(digits = 3)
```

```{r}
bootstrap_results2 = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin, data = df) ),
    results = map(models, broom::tidy)) |> 
  select(-strap_sample, -models) |> 
  unnest(results) 

bootstrap_results2 = bootstrap_results2 |> 
  group_by(strap_number) |> 
  summarize(log_beta = log(
      estimate[term == "(Intercept)"] * estimate[term == "tmin"]
    )
              )
  
bootstrap_results2 %>% 
  summarize(
    log_beta_lower = quantile(log_beta, 0.025), 
    log_beta_upper = quantile(log_beta, 0.975)
  ) %>% 
    knitr::kable(digits = 3)
```

```{r}
ggplot(bootstrap_results1, aes(x = r.squared)) +
  geom_histogram(bins = 30, fill = "green", alpha = 0.7) +
  labs(
    title = "Distribution of Bootstrap Estimates for R-squared",
    x = "R-squared",
    y = "Frequency"
  ) +
  theme_minimal()
```

The distribution appears symmetric and approximately bell-shaped, resembling a normal distribution.This suggests that the variability of $R^2$ values is consistent across the bootstrap samples. The distribution is centered around $R^2$ values between 0.90 and 0.92, indicating that the model consistently explains about 90-92% of the variance in the response variable across resamples.

```{r}
ggplot(bootstrap_results2, aes(x = log_beta)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  labs(
    title = "Distribution of Bootstrap Estimates for log_beta",
    x = "log_beta",
    y = "Frequency"
  ) +
  theme_minimal()
```

The distribution is symmetric and bell-shaped, closely resembling a normal distribution. This indicates that the variability in `log_beta` estimates across bootstrap samples is relatively consistent and unbiased. The expected log-transformed product of the coefficients is approximately 2.
	
# Problem 2

```{r}
homicide_data <- read_csv("homicide-data.csv")
```

```{r warning=FALSE}
homicide_data = homicide_data %>%
  mutate(city_state = paste(city, state, sep = ", ")) %>% 
  mutate(is_solved = ifelse(disposition == "Closed by arrest", 1, 0)) %>% 
  filter(
    !(city_state %in% c("Dallas, TX", "Phoenix, AZ", 
                        "Kansas City, MO", "Tulsa, AL"))
  ) %>%
  filter(victim_race %in% c("White", "Black")) %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  filter(!is.na(victim_age))
```

```{r}
baltimore_data = homicide_data %>%
  filter(city_state == "Baltimore, MD")

logistic_model <- glm(
  is_solved ~ victim_age + victim_sex + victim_race,
  data = baltimore_data,
  family = binomial(link = "logit")  
)

model_summary <- broom::tidy(
  logistic_model, 
  conf.int = TRUE,     
  conf.level = 0.95    
)

odds_ratio <- model_summary %>%
  filter(term == "victim_sexMale") %>%
  mutate(
    odds_ratio = exp(estimate),  
    lower_ci = exp(conf.low),   
    upper_ci = exp(conf.high)    
  ) 

odds_ratio %>% 
  select(odds_ratio:upper_ci) %>% 
  knitr::kable(digits = 3)
```

```{r warning=FALSE}
city_or_results <- homicide_data %>%
  group_by(city_state) %>%  
  nest() %>%  
  mutate(
    model = map(data, ~ glm(is_solved ~ victim_sex + victim_age + victim_race,
                            data = .x, family = binomial(link = "logit"))), 
    tidy_model = purrr::map(model, ~ broom::tidy(.x, conf.int = TRUE, 
                                          conf.level = 0.95)) 
  ) %>%
  unnest(tidy_model) %>%  
  filter(term == "victim_sexMale") %>%  
  mutate(
    odds_ratio = exp(estimate),  
    lower_ci = exp(conf.low),    
    upper_ci = exp(conf.high)    
  ) %>%
  select(city_state, odds_ratio, lower_ci, upper_ci, p.value)  

print(city_or_results)
```

```{r}
ggplot(city_or_results, aes(x = odds_ratio, 
                            y = reorder(city_state, odds_ratio))) +
  geom_point(color = "blue", size = 3) + 
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2, color = "black") +  
  labs(
    title = "Estimated Odds Ratios (ORs) and Confidence Intervals (CIs) by City",
    x = "Odds Ratio (log scale)",
    y = "City"
  ) +
  scale_x_log10() + 
  theme_minimal()
```

The odds ratios range from 0.28 to 2.1 approximately. Cities with odds ratios less than 1 (e.g., Albuquerque, NM) indicate that male victims are less likely to have their homicides solved compared to female victims. Cities with odds ratios greater than 1 (e.g., New York, NY) indicate that male victims are more likely to have their homicides solved compared to female victims. 

A few cities have extreme odds ratios (e.g., New York, NY), suggesting unique dynamics in homicide resolution for male vs. female victims.

# Problem 3

```{r message=FALSE}
birthweight <- read_csv("birthweight.csv", na = c("NA", ".", "")) %>% 
  janitor::clean_names() 
```

```{r message=FALSE}
str(birthweight)
sum(is.na(birthweight))

birthweight <- birthweight %>%
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9),
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8),
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  )
```

I hypothesize that variables including gestational age, mother’s weight gain during pregnancy, baby’s sex, mother’s age, and smoking habits influence birthweight. So I will build regression model based on these factors first.

```{r}
birthweight_model <- lm(
  bwt ~ gaweeks + wtgain + smoken + babysex + momage,
  data = birthweight
)

summary(birthweight_model)

birthweight <- birthweight %>%
  add_predictions(birthweight_model) %>%
  add_residuals(birthweight_model)

ggplot(birthweight, aes(x = pred, y = resid)) +
  geom_point(alpha = 0.6, color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values (Predicted Birthweight)",
    y = "Residuals"
  ) +
  theme_minimal()
```

The spread of residuals appears fairly consistent across the range of fitted values, suggesting no significant heteroscedasticity. A few residuals are far from 0, which might indicate outliers or influential points worth investigating.


Compare my model to two others:

```{r}
model_formulas <- list(
  model1 = bwt ~ gaweeks + wtgain + smoken + babysex + momage,
  model2 = bwt ~ blength + gaweeks,
  model3 = bwt ~ bhead + blength + babysex + bhead * blength * babysex  
)
```

```{r}
set.seed(123)  
cv_data <- crossv_mc(birthweight, 100)

cv_results <- cv_data %>%
  mutate(
    models = map(train, ~ map(model_formulas, lm, data = as.data.frame(.x))),
    
    rmse = map2(models, test, function(model_list, test_set) {
      map_dbl(model_list, function(model) {
        test_set <- as.data.frame(test_set)
        pred <- predict(model, newdata = test_set)
        sqrt(mean((test_set$bwt - pred)^2)) 
      })
    })
  )

rmse_summary <- cv_results %>%
  unnest_longer(rmse) %>%
  mutate(model = rep(names(model_formulas), times = nrow(cv_data))) %>%  
  group_by(model) %>%
  summarize(mean_rmse = mean(rmse), sd_rmse = sd(rmse))

print(rmse_summary)
```

Among these three models, the model using head circumference, length, sex, and all interactions has the lowest mean score of RMSE, indicating it has the best predictive performance.

